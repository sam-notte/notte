# [log]
level = "INFO"
verbose = false
logging_mode = "agent"

# [agent]
# reasoning_model = "gemini/gemini-2.0-flash"
use_vision = true
highlight_elements = true
# max_history_tokens = 32000
max_steps = 20

# [error]
max_error_length = 500
raise_condition = "retry"
max_consecutive_failures = 3

# [browser]
headless = false
browser_type = "chromium"
browser_backend = "patchright"
screenshot_type = "last_action"
web_security = false
solve_captchas = false
# viewport_width = 1920
# viewport_height = 1080
# cdp_url = null
# debug_port = 9222
# user_agent = null
# custom_devtools_frontend = "localhost:9000"
# chrome_args = []

# [proxy]
# proxy_host = null
# proxy_username = null
# proxy_password = null # pragma: allowlist secret

# [llm]
nb_retries_structured_output = 2
nb_retries = 2
clip_tokens=5000
use_llamux = false
temperature = 0.0

# [scraping]
# scraping_model = "gpt-4o-mini"
scraping_type = "markdownify"

# [perception]
# perception_type = "deep"
# perception_model = "cerebras/llama-3.3-70b"


# [dom_parsing]
# Viewport expansion in pixels.
#    This amount will increase the number of elements which are included in the state what the LLM will see.
#    - If set to -1, all elements will be included (this leads to high token usage).
#    - If set to 0, only the elements which are visible in the viewport will be included.
focus_element = -1
viewport_expansion = 0
enable_pointer_elements = true

# [playwright wait/timeout]
timeout_goto_ms        = 10000
timeout_default_ms     =  8000
timeout_action_ms      =  5000
wait_retry_snapshot_ms =  1000
wait_short_ms          =   500
empty_page_max_retry   = 5

# [misc]
enable_profiling = true
